---
layout:     post
title:      "Pitfalls and Solutions When Using Monte Carlo Tree Search for Strategy and Tactical Games"
subtitle:   "GameAIPro3 Chapter28"
date:       2021-07-05  20:29:00
author:     "zhouhd"
header-img: "img/about-bg.jpg"
catalog: true
tags:
    - GameAI
    - GameAIPro
    - GameAIPro3
    - GameProgramming
---

蒙特卡洛树搜索，一些用于策略游戏的具体实现细节和优化

## 概览
- MCTS擅长处理存在大量分支，搜索空间很深的复杂搜索问题，不需要探索所有分支就可以给出一个良好结果
- 最常见的优化是限制搜索分支数量，将一些不太重要的行为直接剔除，这样降低了AI的灵活性和强度
- 先验知识也是一种常见的优化，例如对行为进行优先度分级，优先对重要分支进行搜索
- 下文提到的行为序列是指一整个回合里的一系列的行为；行为集是指行为可取的具体值，例如攻击目标A或者B
- MCTS经典步骤是选择、扩展、模拟和反向传播，下文将分步进行阐述

## 扩展
- 如果一些分支拥有相同的前缀行为，那么应该将这些分支放到同一个分支下，这样可以减少搜索宽度，可以更好找出那些表现好的分支。换句话说，切分行为序列，以行为进行分支，而不是以序列分支
- 优先扩展那些可能表现更好的分支，搜索收敛会更快
- 如果没有启发函数来给分支评分或者需要估计当前启发函数是否足够好，可以用香农熵
- 开始没有任何信息时可以对每个分支选择一个行为作为代表简单评估，不要随机选择，可能产生比较差的结果
- 有时由于时间限制，没有完全搜索完整一整个行为序列，此时可以进行简单补全：模拟阶段时保存模拟过程和结果，补全时选择最好的模拟；根据当前状况重新模拟，补全结果
- 面对特殊情况，可以直接用固定的策略来应对，没有必要进行MCTS
- 如果要处理多个相同角色，可以只计算最强的那个角色，其它角色可以参考它的结果
- 实现上最好写成迭代式计算，这样可以暂停并恢复

## 模拟
- 最简单的随机模拟会使得收敛过程变得很漫长，应该重视模拟的方法
- 可以准备多套可行模拟策略，随机选择其中一种
- 对于策略游戏，可以直接用一些公式计算出分数来评价当前的结果

## 反向传播
- 对一个分支模拟完毕之后，估值函数会根据结果，对当前路径上的节点修改效用评分
- 估值函数应该能比较好地区分各种行为的结果，并且能够让搜索快速收敛

## 选择
- 如果对一个节点的分支进行过一定次数的搜索后，可以先进行一次排序，这样就基本有序了。后续更新分支时可以简单用插入排序
